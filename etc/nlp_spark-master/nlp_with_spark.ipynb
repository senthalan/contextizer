{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "# my local spark install\n",
    "findspark.init('/Users/dreyco676/spark-1.6.0-bin-hadoop2.6/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# create spark contexts\n",
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import preproc as pp\n",
    "# Register all the functions in Preproc with Spark Context\n",
    "check_lang_udf = udf(pp.check_lang, StringType())\n",
    "remove_stops_udf = udf(pp.remove_stops, StringType())\n",
    "remove_features_udf = udf(pp.remove_features, StringType())\n",
    "tag_and_remove_udf = udf(pp.tag_and_remove, StringType())\n",
    "lemmatize_udf = udf(pp.lemmatize, StringType())\n",
    "check_blanks_udf = udf(pp.check_blanks, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row.\n",
    "data_rdd = sc.textFile(\"data/raw_classified.txt\")\n",
    "parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "# Filter bad rows out\n",
    "garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "# Create DataFrame\n",
    "data_df = sqlContext.createDataFrame(typed_rdd, [\"text\", \"id\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict language and filter out those with less than 90% chance of being English\n",
    "lang_df = data_df.withColumn(\"lang\", check_lang_udf(data_df[\"text\"]))\n",
    "en_df = lang_df.filter(lang_df[\"lang\"] == \"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words to reduce dimensionality\n",
    "rm_stops_df = en_df.withColumn(\"stop_text\", remove_stops_udf(en_df[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove other non essential words, think of it as my personal stop word list\n",
    "rm_features_df = rm_stops_df.withColumn(\"feat_text\", remove_features_udf(rm_stops_df[\"stop_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag the words remaining and keep only Nouns, Verbs and Adjectives\n",
    "tagged_df = rm_features_df.withColumn(\"tagged_text\", tag_and_remove_udf(rm_features_df[\"feat_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of remaining words to reduce dimensionality & boost measures\n",
    "lemm_df = tagged_df.withColumn(\"lemm_text\", lemmatize_udf(tagged_df[\"tagged_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows containing only blank spaces\n",
    "check_blanks_df = lemm_df.withColumn(\"is_blank\", check_blanks_udf(lemm_df[\"lemm_text\"]))\n",
    "no_blanks_df = check_blanks_df.filter(check_blanks_df[\"is_blank\"] == \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "no_blanks_df.withColumnRenamed(no_blanks_df[\"lemm_text\"], \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dedupe important since alot of the tweets only differed by url's and RT mentions\n",
    "dedup_df = no_blanks_df.dropDuplicates(['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the columns we care about\n",
    "data_set = dedup_df.select(dedup_df['id'], dedup_df['text'], dedup_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training & validation sets with 60% to training and use a seed value of 1987\n",
    "splits = data_set.randomSplit([0.6, 0.4])\n",
    "training_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "#\n",
    "#   Spark ML Section\n",
    "#   \n",
    "#   Skip Preprocessing and use cleaned files by running next cell\n",
    "#\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load already cleaned data\n",
    "def reload_checkpoint(data_rdd):\n",
    "    parts_rdd = data_rdd.map(lambda l: l.split(\"\\t\"))\n",
    "    # Filter bad rows out\n",
    "    garantee_col_rdd = parts_rdd.filter(lambda l: len(l) == 3)\n",
    "    typed_rdd = garantee_col_rdd.map(lambda p: (p[0], p[1], float(p[2])))\n",
    "    # Create DataFrame\n",
    "    df = sqlContext.createDataFrame(typed_rdd, [\"id\", \"text\", \"label\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load precleaned training set\n",
    "training_rdd = sc.textFile(\"data/clean_training.txt\")\n",
    "training_df = reload_checkpoint(training_rdd)\n",
    "# Load precleaned test set\n",
    "test_rdd = sc.textFile(\"data/clean_test.txt\")\n",
    "test_df = reload_checkpoint(test_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and nb.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "idf = IDF(minDocFreq=3, inputCol=\"features\", outputCol=\"idf\")\n",
    "nb = NaiveBayes()\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 1.0]).build()\n",
    "\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    evaluator=MulticlassClassificationEvaluator(), \n",
    "                    numFolds=4)\n",
    "\n",
    "cvModel = cv.fit(training_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cvModel.transform(test_df)\n",
    "prediction_df = result.select(\"text\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasci_df = prediction_df.filter(prediction_df['label']==0.0)\n",
    "datasci_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_df = prediction_df.filter(prediction_df['label']==1.0)\n",
    "ao_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9228856806385485"
      ]
     },
     "execution_count": 12,
     "output_type": "execute_result",
     "metadata": {}
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(result, {evaluator.metricName: \"precision\"})"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
